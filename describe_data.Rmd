---
title: 'Appendix: Understanding and Predicting Web Content Credibility'
author: "Michał Kąkol"
date: "24 02 2017"
output:
  html_notebook:
    code_folding: hide
    number_sections: yes
    toc: yes
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---
```{R,echo=FALSE,messages=FALSE,warning=FALSE}
library(dplyr)
library(ggplot2)
options(digits=2)
```

# This appendix motivation

This is a data set report supplementing the article "Understanding and Predicting Web Content Credibility: Newly Identified Factors". The main purpose of this report is to describe the data set covered in the article in more detail thus extending the limited space of the publication.

# Download

* This data set itself can be downloaded [here](https://github.com/s8811/reconcile-tags/raw/master/csv/imported.RData)

# MTurk labeling interface

* The interface the crowdsourcing workers used to label the textual justification can be previewed [here](https://rawgit.com/s8811/reconcile-tags/master/tagging-interface/mturk_preview.html).

# Tables

This section describes data tables. The relations between the tables and records within is depicted in the diagram below. The flow of the data as they were gathers was as follows. In a separate study a set of web pages (pages) was evaluated in terms of their (inter alia) credibility and a textual justification of this evaluation was gathered accordingly (evaluations). In the study covered by the article "Understanding and Predicting Web Content Credibility" the textual justications (evaluations) were labeled in a crowdsourcing setting by multiple labelers (labelers), thus providing a description of the evaluations and their corresponding web pages with a set of labels representing possibe credibility related web page and content issues (evaluations_labelings and evaluations_labelings_exploded).

![Tables relations](reconcile-tags-diagram.png)

## Evaluations

This table consists of raters' evaluations of the web pages they were presented. The rater was presented with the archived version of the web site and asked to evaluate it in several dimensions as well as leave a textual justification of the evaluation outcome.

This table consists of `r dim(c3.evaluations)` records, which is a number of credibility evaluations gathered in [Reconcile](http://reconcie.pjwstk.edu.pl) project for the day the tagging data set was separetd from the original data set. However the number of evaluations labeled in this study is `r c3.evaluations %>%
  semi_join(c3.labelings,by=c("documentevaluation_id")) %>%
  select(document_id) %>%
  unique %>%
  nrow`.

* **rater_id**, \<fctr\\> - ID of a person that evaluated the web page
* **documentevaluation_id**, \<fctr\> - ID of the aforementioned evaluation
* **document_id**, \<fctr\> - ID of the web page
* **documentevaluation_start**, \<dttm\> - timestamp opening the evaluation
* **documentevaluation_end**, \<dttm\> - timestamp closing the evaluation
* **documentevaluation_description**, \<chr\> -  tags describing the web page contents, left by the reater
* **documentevaluation_comment**, \<chr\> - rater's textual justification of the evaluation outcome
* **documentevaluation_website1**, \<chr\> - web site URL related to evaluation of the web page
* **documentevaluation_website2**, \<chr\> - vide supra
* **documentevaluation_website3**, \<chr\> - vide supra
* **documentevaluation_credibility**, \<int\> - web page credibility score
* **documentevaluation_presentation**, \<int\> - web page presentation score
* **documentevaluation_knowledge**, \<int\> - web page author knowledge score
* **documentevaluation_intentions**, \<int\> - web page author intentions score
* **documentevaluation_completeness**, \<int\> - web page completeness score
* **documentevaluation_experience**, \<lgl\> - rater experience with web page subject score
* **documentevaluation_opinion**, \<lgl\> - rater opinion 
* **documentevaluation_knowledgeable**, \<lgl\> - 
* **documentevaluation_baddescription** \<lgl\> - whether the evaluation has errorneous tag description
* **documentevaluation_badcomment**, \<lgl\> - whether the evaluation has errorneous textual justification
* **documentevaluation_badwebsites**, \<lgl\> - whether the evaluation has errorneous related web pages
* **personexperimentround_id**, \<fctr\> - ID of the batch of corresponding **documentevaluation_id**

```{r}
head(c3.evaluations)
```

## Golden examples

This section describes the golden examples used for validating the rater' work. There was at least 2 golden examples per possible label.

* **documentevaluation_id**, <fctr> - ID of the golden example
* **tag_code**, <fctr> - ID of the tag being validated
* **gold_text**, <chr> - golden example text
* **tag_label**, <chr> - related tag label
* **tag_group**, <fctr> - (temporary) tag group

```{R}
c3.golden %>%
  select(tag_code,tag_label,tag_group,gold_text)
```

## Labelers

This section describes the labelers table. Each row is a labeler (person_id), i.e. MTurk job participant tagging the web site evaluation textual justifications.

* **person_id**, \<fctr\> - internal ID of a MTurk worker doing labelling tasks
* **person_user_name**, \<chr\> - MTurk user ID
* **is_test**, \<lgl\> - is this a test user
* **person_gender**, \<fctr\> - user's gender
* **person_birthyr**, \<fctr\> - user's birthyear (1970 was a default value, should be read as NA)
* **person_education**, \<fctr\> - user's education level
* **person_wage**, \<fctr\> - user's wage
* **person_politics**, \<fctr\> - user's political preferences score (left to right)
* **person_state**, \<fctr\> - user's US state of residence
* **person_country**, \<fctr\> - user's country of residence

The described data set covers labeling results from `r dim(c3.labelers)[1]` labelers of which `r dim(c3.labelers[c3.labelers$is_test==TRUE,])[1]` were test users, for platform testing purposes only.

Despite the placeholders for the demographic data about the labelers the percentage of the labeling MTurk tasks that did not fill in this questionaire amounted to `r dim(c3.labelers[c3.labelers$person_birthyr=='1970',])[1]/dim(c3.labelers)[1] * 100`% that is why these results are not going to be described here. 


## Labelings

The "Labelings" table consists of rows where each row is a labeling (commentlabeling_id) of a single web page credibility evaluation textual justification (documentevaluation_id) from evaluator (rater_id) refering to a web site (document_id).

* **commentlabeling_id**, \<fctr\> - ID of labeling of a single textual credibility score justification
* **person_id**, \<fctr\> - ID of a labeler
* **rater_id**, \<fctr\> - ID of rater providing credibility score textual justification
* **documentevaluation_id**, \<fctr\> - ID of a web page evaluation task
* **document_id**, \<fctr\> - ID of a web site related to evaluation task
* **start**, \<dttm\> - task start timestamp
* **end**, \<dttm\> - task end timestamp
* **answer**, \<chr\> - provided labels set (tags)
* **rejected**, \<lgl\> - whether labeling was rejected due to not passing validation
* **is_test**, \<lgl\> - whether labeling was a test task
* **is_gold**, \<lgl\> - whether labeling was a validation step
* **is_finished**, \<lgl\> - whether labeling was successfully finished
* **is_ok**, \<lgl\> - whether this labeling recordcan be used for further analysis (not test, sucessfuly finished and validated)

`r c3.labelings %>%
  select(document_id) %>%
  unique %>%
  as.vector %>% 
  nrow` documents were labeled by `r c3.labelings %>%
  select(rater_id) %>%
  unique %>%
  as.vector %>% 
  nrow` raters in `r c3.labelings %>%
  select(commentlabeling_id) %>%
  unique %>%
  as.vector %>% 
  nrow` labeling tasks.

Summary of the labeling duration is shown below (in seconds)
```{r}
temp <- c3.labelings
temp <- temp[temp$is_ok==TRUE,]
#hist(as.numeric(temp$end-temp$start),breaks = 100)
summary(as.numeric(temp$end-temp$start))
```

## Labelings (exploded)

* **commentlabeling_id**, \<fctr\> - ID of labeling of a single textual credibility score justification
* **person_id**, \<fctr\> - ID of a labeler
* **documentevaluation_id**, \<fctr\> - ID of a web page evaluation task
* **document_id**, \<fctr\> - ID of a web site related to evaluation task
* **category**, \<chr\> - thematic category of a related web page
* **credibility**, \<int\> - credibility score (aggregated and binned) of the related web page
* **label**, \<fctr\> - a single tag code

Below is a table representing number of times certain label was assigned to all documentevaluation record.
```{r}
as.data.frame(prop.table(table(c3.labelings_exploded[!is.na(c3.labelings_exploded$label),"label"]))) %>%
  rename(tag_code=Var1,Percentage=Freq) %>%
  inner_join(c3.tags)
```

## Sites

* **document_id**, \<fctr\> - web page ID
* **document_url**, \<chr\> - web page url
* **document_category_id**, \<fctr\> - web page thematic category ID
* **document_children**, \<int\> - number of archived children web pages 
* **document_source_type**, \<fctr\> - ID of a web pages acquisition step
* **document_source_name**, \<fctr\> - (for web pages acquiried from Google) a related Google query
* **document_category_name**, \<fctr\> -  web page thematic category label

There is `r c3.sites %>%
  select(document_id) %>% 
  unique %>% 
  nrow` documents from which `r c3.sites %>%
  semi_join(c3.labelings,by=c("document_id")) %>%
  select(document_id) %>% 
  unique %>% 
  nrow` were labeled and are considered to come from following thematic categories, shown in table below:

```{r}
c3.sites %>%
  semi_join(c3.labelings,by=c("document_id")) %>%
  select(document_category_name) %>%
  table %>%
  prop.table %>%
  as.data.frame %>%
  rename(Percentage=Freq)
```

## Tags
Tags are labels used by labelers to describe raters reasons to evaluate a web page high or low. 

* **tag_code**, \<fctr\> - label ID
* **tag_label**, \<chr\> - full label name
* **tag_group**, \<fctr\> - (temporary) label group

```{r}
c3.tags
```


