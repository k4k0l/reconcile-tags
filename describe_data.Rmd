---
title: 'Appendix: Understanding and Predicting Web Content Credibility'
author: "Michał Kąkol"
date: "24 02 2017"
output:
  html_notebook:
    code_folding: hide
    number_sections: yes
    toc: yes
  pdf_document:
    toc: yes
---
```{R,echo=FALSE,messages=FALSE,warning=FALSE}
library(dplyr)
library(ggplot2)
options(digits=2)
```

# This appendix motivation

This is a data set report supplementing the article "Understanding and Predicting Web Content Credibility: Newly Identified Factors". The main purpose of this report is to describe the data set covered in the article in more detail thus extending the limited space of the publication.

OPIS RELACJI MIĘDZY TABELAMI
 -> JAK ZBIERANO DANE, OPIS PROCESU

LINK DO MOCKUPU 

# Tables

This section describes data set tables.

## Evaluations

This table consists of raters' evaluations of the web pages they were presented. The rater was presented with the archived version of the web site and asked to evaluate it in several dimensions as well as leave a textual justification of the evaluation outcome.

* **rater_id**, \<fctr\\> - ID of a person that evaluated the web page
* **documentevaluation_id**, \<fctr\> - ID of the aforementioned evaluation
* **document_id**, \<fctr\> - ID of the web page
* **documentevaluation_start**, \<dttm\> - timestamp opening the evaluation
* **documentevaluation_end**, \<dttm\> - timestamp closing the evaluation
* **documentevaluation_description**, \<chr\> -  tags describing the web page contents, left by the reater
* **documentevaluation_comment**, \<chr\> - rater's textual justification of the evaluation outcome
* **documentevaluation_website1**, \<chr\> - web site URL related to evaluation of the web page
* **documentevaluation_website2**, \<chr\> - vide supra
* **documentevaluation_website3**, \<chr\> - vide supra
* **documentevaluation_credibility**, \<int\> - web page credibility score
* **documentevaluation_presentation**, \<int\> - web page presentation score
* **documentevaluation_knowledge**, \<int\> - web page author knowledge score
* **documentevaluation_intentions**, \<int\> - web page author intentions score
* **documentevaluation_completeness**, \<int\> - web page completeness score
* **documentevaluation_experience**, \<lgl\> - rater experience with web page subject score
* **documentevaluation_opinion**, \<lgl\> - rater opinion 
* **documentevaluation_knowledgeable**, \<lgl\> - 
* **documentevaluation_baddescription** \<lgl\> - whether the evaluation has errorneous tag description
* **documentevaluation_badcomment**, \<lgl\> - whether the evaluation has errorneous textual justification
* **documentevaluation_badwebsites**, \<lgl\> - whether the evaluation has errorneous related web pages
* **personexperimentround_id**, \<fctr\> - ID of the batch of corresponding **documentevaluation_id**

```{r}
head(c3.evaluations)
```

## Golden examples

This section describes the golden examples used for validating the rater' work.

* **documentevaluation_id**, <fctr> - ID of the golden example
* **tag_code**, <fctr> - ID of the tag being validated
* **gold_text**, <chr> - golden example text
* **tag_label**, <chr> - related tag label
* **tag_group**, <fctr> - (temporary) tag group

```{R}
c3.golden %>%
  select(tag_code,tag_label,tag_group,gold_text)
```

## Labelers

This section describes the labelers table. Each row is a labeler (person_id), i.e. MTurk job participant tagging the web site evaluation textual justifications.

* **person_id**, \<fctr\> - internal ID of a MTurk worker doing labelling tasks
* **person_user_name**, \<chr\> - MTurk user ID
* **is_test**, \<lgl\> - is this a test user
* **person_gender**, \<fctr\> - user's gender
* **person_birthyr**, \<fctr\> - user's birthyear (1970 was a default value, should be read as NA)
* **person_education**, \<fctr\> - user's education level
* **person_wage**, \<fctr\> - user's wage
* **person_politics**, \<fctr\> - user's political preferences score (left to right)
* **person_state**, \<fctr\> - user's US state of residence
* **person_country**, \<fctr\> - user's country of residence

The described data set covers labeling results from `r dim(c3.labelers)[1]` labelers of which `r dim(c3.labelers[c3.labelers$is_test==TRUE,])[1]` were test users, for platform testing purposes only.

Despite the placeholders for the demographic data about the labelers the percentage of the labeling MTurk tasks that did not fill in this questionaire amounted to `r dim(c3.labelers[c3.labelers$person_birthyr=='1970',])[1]/dim(c3.labelers)[1] * 100`% that is why these results are not going to be described here. 


## Labelings

The "Labelings" table consists of rows where each row is a labeling (commentlabeling_id) of a single web page credibility evaluation textual justification (documentevaluation_id) from evaluator (rater_id) refering to a web site (document_id).

* **commentlabeling_id**, \<fctr\> - ID of labeling of a single textual credibility score justification
* **person_id**, \<fctr\> - ID of a labeler
* **rater_id**, \<fctr\> - ID of rater providing credibility score textual justification
* **documentevaluation_id**, \<fctr\> - ID of a web page evaluation task
* **document_id**, \<fctr\> - ID of a web site related to evaluation task
* **start**, \<dttm\> - task start timestamp
* **end**, \<dttm\> - task end timestamp
* **answer**, \<chr\> - provided labels set (tags)
* **rejected**, \<lgl\> - whether labeling was rejected due to not passing validation
* **is_test**, \<lgl\> - whether labeling was a test task
* **is_gold**, \<lgl\> - whether labeling was a validation step
* **is_finished**, \<lgl\> - whether labeling was successfully finished
* **is_ok**, \<lgl\> - whether this labeling recordcan be used for further analysis (not test, sucessfuly finished and validated)



Summary of the labeling duration is shown below (in seconds)
```{r}
temp <- c3.labelings
temp <- temp[temp$is_ok==TRUE,]
#hist(as.numeric(temp$end-temp$start),breaks = 100)
summary(as.numeric(temp$end-temp$start))
```

## Labelings (exploded)

* **commentlabeling_id**, \<fctr\> - ID of labeling of a single textual credibility score justification
* **person_id**, \<fctr\> - ID of a labeler
* **documentevaluation_id**, \<fctr\> - ID of a web page evaluation task
* **document_id**, \<fctr\> - ID of a web site related to evaluation task
* **category**, \<chr\> - thematic category of a related web page
* **credibility**, \<int\> - credibility score (aggregated and binned) of the related web page
* **label**, \<fctr\> - a single tag code

```{r}
unique(c3.labelings_exploded$label)
```

```{r}
?rename
as.data.frame(prop.table(table(c3.labelings_exploded[!is.na(c3.labelings_exploded$label),"label"]))*100) %>%
  rename(tag_code=Var1,Percentage=Freq) %>%
  inner_join(c3.tags)
```

```{r}
temp <- c3.labelings[c3.labelings$is_ok == TRUE,]
temp$duration <- temp$end-temp$start
summary(as.numeric(temp$duration))
hist(as.numeric(temp$end-temp$start),breaks = 100,)
```

## Sites

* **document_id**, \<fctr\> - web page ID
* **document_url**, \<chr\> - web page url
* **document_category_id**, \<fctr\> - web page thematic category ID
* **document_children**, \<int\> - number of archived children web pages 
* **document_source_type**, \<fctr\> - ID of a web pages acquisition step
* **document_source_name**, \<fctr\> - (for web pages acquiried from Google) a related Google query
* **document_category_name**, \<fctr\> -  web page thematic category label

## Tags

* **tag_code**, \<fctr\> - label ID
* **tag_label**, \<chr\> - full label name
* **tag_group**, \<fctr\> - (temporary) label group

```{r}
c3.tags
```


